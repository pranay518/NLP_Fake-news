{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e63ce242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b43be2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"\n",
    "AI tools like ChatGPT, Bard, and Bing AI have become increasingly popular \n",
    "and powerful. However, they can also be a security risk. At the moment, the \n",
    "Sandvik proposal is to not forbid but to follow specific recommendations. \n",
    "Since the field of AI is rapidly evolving and globally discussed, we will provide \n",
    "more information as soon as we have it.\n",
    "Using third-party AI tools can cause security risks. This has lead companies like \n",
    "Samsung to ban their use for employees and even countries to totally forbid Chat \n",
    "GPT, like Italy. Given the uncertainty surrounding data use in AI tools, it is \n",
    "strongly advised to use caution, specifically in a work-related context. \n",
    "Therefore, the following is recommended:\n",
    "1. Do not regard AI services outputs as a definitive source of truth. AI \n",
    "models can provide incorrect or outdated information and are prone \n",
    "to errors.\n",
    "2. Do not include any company information or someone's personal \n",
    "data in AI services that is not considered safe to share externally, \n",
    "meaning you should treat interactions with AI services as if you \n",
    "were talking to an external individual. AI services' high capabilities \n",
    "comes from their training, with both input and output data used for \n",
    "future training. It's important to be cautious when sharing \n",
    "information - simply masking or disguise sensitive data isn't enough.\n",
    "3. Choose to not allow your data to be used for training by the AI tools \n",
    "(“opt out\"). This is not possible for all tools, but more information \n",
    "about how-to at the end of this article.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30af6558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAI tools like ChatGPT, Bard, and Bing AI have become increasingly popular \\nand powerful. However, they can also be a security risk. At the moment, the \\nSandvik proposal is to not forbid but to follow specific recommendations. \\nSince the field of AI is rapidly evolving and globally discussed, we will provide \\nmore information as soon as we have it.\\nUsing third-party AI tools can cause security risks. This has lead companies like \\nSamsung to ban their use for employees and even countries to totally forbid Chat \\nGPT, like Italy. Given the uncertainty surrounding data use in AI tools, it is \\nstrongly advised to use caution, specifically in a work-related context. \\nTherefore, the following is recommended:\\n1. Do not regard AI services outputs as a definitive source of truth. AI \\nmodels can provide incorrect or outdated information and are prone \\nto errors.\\n2. Do not include any company information or someone\\'s personal \\ndata in AI services that is not considered safe to share externally, \\nmeaning you should treat interactions with AI services as if you \\nwere talking to an external individual. AI services\\' high capabilities \\ncomes from their training, with both input and output data used for \\nfuture training. It\\'s important to be cautious when sharing \\ninformation - simply masking or disguise sensitive data isn\\'t enough.\\n3. Choose to not allow your data to be used for training by the AI tools \\n(“opt out\"). This is not possible for all tools, but more information \\nabout how-to at the end of this article.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53fb8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4430d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokennization - convert paragraph-sentences-words\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e14e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ad9e184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7e3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07940484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drink'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('drinking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d854edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71bf07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6500b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45712d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1076ad52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b6fd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69674e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=[]\n",
    "# for i in range (len(sentences)):\n",
    "#     review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "#     review=review.lower()\n",
    "#     review=review.split()\n",
    "#     corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d77831da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "828c0a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ed0b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in corpus:\n",
    "#     text = str(i)\n",
    "#     words=nltk.word_tokenize(text)\n",
    "#     for word in words:\n",
    "#         if word not in set(stopwords.words('english')):\n",
    "#             print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0259fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Lemmatization\n",
    "# for i in corpus:\n",
    "#     text = str(i)\n",
    "#     words=nltk.word_tokenize(text)\n",
    "#     for word in words:\n",
    "#         if word not in set(stopwords.words('english')):\n",
    "#             print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67176ba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#conda update -n base -c conda-forge conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48f4d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3587532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3399246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords & Lammatization\n",
    "import re\n",
    "corpus = []\n",
    "for i in range (len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if  word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ad2aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(binary=True,ngram_range=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eae23a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = [' '.join(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98e696bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63cae4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai tool': 13,\n",
       " 'tool like': 204,\n",
       " 'like chatgpt': 121,\n",
       " 'chatgpt bard': 40,\n",
       " 'bard bing': 24,\n",
       " 'bing ai': 28,\n",
       " 'ai become': 2,\n",
       " 'become increasingly': 26,\n",
       " 'increasingly popular': 104,\n",
       " 'popular powerful': 144,\n",
       " 'ai tool like': 15,\n",
       " 'tool like chatgpt': 205,\n",
       " 'like chatgpt bard': 122,\n",
       " 'chatgpt bard bing': 41,\n",
       " 'bard bing ai': 25,\n",
       " 'bing ai become': 29,\n",
       " 'ai become increasingly': 3,\n",
       " 'become increasingly popular': 27,\n",
       " 'increasingly popular powerful': 105,\n",
       " 'however also': 96,\n",
       " 'also security': 20,\n",
       " 'security risk': 165,\n",
       " 'however also security': 97,\n",
       " 'also security risk': 21,\n",
       " 'moment sandvik': 132,\n",
       " 'sandvik proposal': 163,\n",
       " 'proposal forbid': 148,\n",
       " 'forbid follow': 85,\n",
       " 'follow specific': 80,\n",
       " 'specific recommendation': 187,\n",
       " 'moment sandvik proposal': 133,\n",
       " 'sandvik proposal forbid': 164,\n",
       " 'proposal forbid follow': 149,\n",
       " 'forbid follow specific': 86,\n",
       " 'follow specific recommendation': 81,\n",
       " 'since field': 182,\n",
       " 'field ai': 78,\n",
       " 'ai rapidly': 6,\n",
       " 'rapidly evolving': 154,\n",
       " 'evolving globally': 73,\n",
       " 'globally discussed': 90,\n",
       " 'discussed provide': 64,\n",
       " 'provide information': 152,\n",
       " 'information soon': 114,\n",
       " 'since field ai': 183,\n",
       " 'field ai rapidly': 79,\n",
       " 'ai rapidly evolving': 7,\n",
       " 'rapidly evolving globally': 155,\n",
       " 'evolving globally discussed': 74,\n",
       " 'globally discussed provide': 91,\n",
       " 'discussed provide information': 65,\n",
       " 'provide information soon': 153,\n",
       " 'using third': 229,\n",
       " 'third party': 198,\n",
       " 'party ai': 140,\n",
       " 'tool cause': 200,\n",
       " 'cause security': 32,\n",
       " 'using third party': 230,\n",
       " 'third party ai': 199,\n",
       " 'party ai tool': 141,\n",
       " 'ai tool cause': 14,\n",
       " 'tool cause security': 201,\n",
       " 'cause security risk': 33,\n",
       " 'lead company': 119,\n",
       " 'company like': 48,\n",
       " 'like samsung': 124,\n",
       " 'samsung ban': 161,\n",
       " 'ban use': 22,\n",
       " 'use employee': 223,\n",
       " 'employee even': 68,\n",
       " 'even country': 71,\n",
       " 'country totally': 52,\n",
       " 'totally forbid': 209,\n",
       " 'forbid chat': 83,\n",
       " 'chat gpt': 38,\n",
       " 'gpt like': 92,\n",
       " 'like italy': 123,\n",
       " 'lead company like': 120,\n",
       " 'company like samsung': 49,\n",
       " 'like samsung ban': 125,\n",
       " 'samsung ban use': 162,\n",
       " 'ban use employee': 23,\n",
       " 'use employee even': 224,\n",
       " 'employee even country': 69,\n",
       " 'even country totally': 72,\n",
       " 'country totally forbid': 53,\n",
       " 'totally forbid chat': 210,\n",
       " 'forbid chat gpt': 84,\n",
       " 'chat gpt like': 39,\n",
       " 'gpt like italy': 93,\n",
       " 'given uncertainty': 88,\n",
       " 'uncertainty surrounding': 217,\n",
       " 'surrounding data': 192,\n",
       " 'data use': 57,\n",
       " 'use ai': 219,\n",
       " 'tool strongly': 207,\n",
       " 'strongly advised': 190,\n",
       " 'advised use': 0,\n",
       " 'use caution': 221,\n",
       " 'caution specifically': 34,\n",
       " 'specifically work': 188,\n",
       " 'work related': 231,\n",
       " 'related context': 158,\n",
       " 'given uncertainty surrounding': 89,\n",
       " 'uncertainty surrounding data': 218,\n",
       " 'surrounding data use': 193,\n",
       " 'data use ai': 58,\n",
       " 'use ai tool': 220,\n",
       " 'ai tool strongly': 17,\n",
       " 'tool strongly advised': 208,\n",
       " 'strongly advised use': 191,\n",
       " 'advised use caution': 1,\n",
       " 'use caution specifically': 222,\n",
       " 'caution specifically work': 35,\n",
       " 'specifically work related': 189,\n",
       " 'work related context': 232,\n",
       " 'therefore following': 196,\n",
       " 'following recommended': 82,\n",
       " 'therefore following recommended': 197,\n",
       " 'regard ai': 156,\n",
       " 'ai service': 8,\n",
       " 'service output': 172,\n",
       " 'output definitive': 138,\n",
       " 'definitive source': 62,\n",
       " 'source truth': 186,\n",
       " 'regard ai service': 157,\n",
       " 'ai service output': 11,\n",
       " 'service output definitive': 173,\n",
       " 'output definitive source': 139,\n",
       " 'definitive source truth': 63,\n",
       " 'ai model': 4,\n",
       " 'model provide': 130,\n",
       " 'provide incorrect': 150,\n",
       " 'incorrect outdated': 102,\n",
       " 'outdated information': 134,\n",
       " 'information prone': 108,\n",
       " 'prone error': 147,\n",
       " 'ai model provide': 5,\n",
       " 'model provide incorrect': 131,\n",
       " 'provide incorrect outdated': 151,\n",
       " 'incorrect outdated information': 103,\n",
       " 'outdated information prone': 135,\n",
       " 'information prone error': 109,\n",
       " 'include company': 100,\n",
       " 'company information': 46,\n",
       " 'information someone': 112,\n",
       " 'someone personal': 184,\n",
       " 'personal data': 142,\n",
       " 'data ai': 54,\n",
       " 'service considered': 168,\n",
       " 'considered safe': 50,\n",
       " 'safe share': 159,\n",
       " 'share externally': 176,\n",
       " 'externally meaning': 76,\n",
       " 'meaning treat': 128,\n",
       " 'treat interaction': 215,\n",
       " 'interaction ai': 117,\n",
       " 'service talking': 174,\n",
       " 'talking external': 194,\n",
       " 'external individual': 75,\n",
       " 'include company information': 101,\n",
       " 'company information someone': 47,\n",
       " 'information someone personal': 113,\n",
       " 'someone personal data': 185,\n",
       " 'personal data ai': 143,\n",
       " 'data ai service': 55,\n",
       " 'ai service considered': 9,\n",
       " 'service considered safe': 169,\n",
       " 'considered safe share': 51,\n",
       " 'safe share externally': 160,\n",
       " 'share externally meaning': 177,\n",
       " 'externally meaning treat': 77,\n",
       " 'meaning treat interaction': 129,\n",
       " 'treat interaction ai': 216,\n",
       " 'interaction ai service': 118,\n",
       " 'ai service talking': 12,\n",
       " 'service talking external': 175,\n",
       " 'talking external individual': 195,\n",
       " 'service high': 170,\n",
       " 'high capability': 94,\n",
       " 'capability come': 30,\n",
       " 'come training': 44,\n",
       " 'training input': 213,\n",
       " 'input output': 115,\n",
       " 'output data': 136,\n",
       " 'data used': 59,\n",
       " 'used future': 225,\n",
       " 'future training': 87,\n",
       " 'ai service high': 10,\n",
       " 'service high capability': 171,\n",
       " 'high capability come': 95,\n",
       " 'capability come training': 31,\n",
       " 'come training input': 45,\n",
       " 'training input output': 214,\n",
       " 'input output data': 116,\n",
       " 'output data used': 137,\n",
       " 'data used future': 60,\n",
       " 'used future training': 226,\n",
       " 'important cautious': 98,\n",
       " 'cautious sharing': 36,\n",
       " 'sharing information': 178,\n",
       " 'information simply': 110,\n",
       " 'simply masking': 180,\n",
       " 'masking disguise': 126,\n",
       " 'disguise sensitive': 66,\n",
       " 'sensitive data': 166,\n",
       " 'data enough': 56,\n",
       " 'important cautious sharing': 99,\n",
       " 'cautious sharing information': 37,\n",
       " 'sharing information simply': 179,\n",
       " 'information simply masking': 111,\n",
       " 'simply masking disguise': 181,\n",
       " 'masking disguise sensitive': 127,\n",
       " 'disguise sensitive data': 67,\n",
       " 'sensitive data enough': 167,\n",
       " 'choose allow': 42,\n",
       " 'allow data': 18,\n",
       " 'used training': 227,\n",
       " 'training ai': 211,\n",
       " 'tool opt': 206,\n",
       " 'choose allow data': 43,\n",
       " 'allow data used': 19,\n",
       " 'data used training': 61,\n",
       " 'used training ai': 228,\n",
       " 'training ai tool': 212,\n",
       " 'ai tool opt': 16,\n",
       " 'possible tool': 145,\n",
       " 'tool information': 202,\n",
       " 'information end': 106,\n",
       " 'end article': 70,\n",
       " 'possible tool information': 146,\n",
       " 'tool information end': 203,\n",
       " 'information end article': 107}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06dee938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai tool like chatgpt bard bing ai become increasingly popular powerful'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "abe2dbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6398d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - tfid\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.anaconda.cloud/repo/main/win-64\n",
      "  - https://repo.anaconda.cloud/repo/main/noarch\n",
      "  - https://repo.anaconda.cloud/repo/r/win-64\n",
      "  - https://repo.anaconda.cloud/repo/r/noarch\n",
      "  - https://repo.anaconda.cloud/repo/msys2/win-64\n",
      "  - https://repo.anaconda.cloud/repo/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install Tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "54866a3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (C:\\Users\\INPUNPRAP\\.conda\\envs\\NLP_Practice\\lib\\site-packages\\sklearn\\feature_extraction\\text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TFIDF:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidVectorizer\n\u001b[0;32m      3\u001b[0m cv \u001b[38;5;241m=\u001b[39m TfidVectorizer()\n\u001b[0;32m      4\u001b[0m X\u001b[38;5;241m=\u001b[39mcv\u001b[38;5;241m.\u001b[39mfit_transform(Corpus)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (C:\\Users\\INPUNPRAP\\.conda\\envs\\NLP_Practice\\lib\\site-packages\\sklearn\\feature_extraction\\text.py)"
     ]
    }
   ],
   "source": [
    "# TFIDF:\n",
    "from sklearn.feature_extraction.text import TfidVectorizer\n",
    "cv = TfidVectorizer()\n",
    "X=cv.fit_transform(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2183903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
